{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhBpZOCWnJ0d54tTxwnE4d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","_ = torch.manual_seed(1)"],"metadata":{"id":"fBVEtUBSyeWk","executionInfo":{"status":"ok","timestamp":1688706624573,"user_tz":-540,"elapsed":325,"user":{"displayName":"곽성수","userId":"13238959409532928202"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### Univariate Linear Regression\n","\n","Hypothesis: $$ H(x) = Wx + b $$\n","Cost Function: $$ Cost = {1 \\over N} \\sum_{i=1}^{N} {(Wx_i + b - y_i) ^ 2} $$\n","Gradient of W: $$ dW = {2 \\over N} \\sum_{i=1}^{N} {(Wx_i + b - y_i)} \\text{ } {x_i}$$\n","$$ \\text{First } dW = {2 \\over 3} {((-1) \\cdot 2 + (-2) \\cdot 4 + (-3) \\cdot 6)} = -18.667$$\n","\n","Gradient of b: $$ db = {2 \\over N} \\sum_{i=1}^{N} {(Wx_i + b - y_i)} $$\n","$$ \\text{First } db = {2 \\over 3} {((-2)+ (-4) + (-6))} = -8.000$$"],"metadata":{"id":"49-XG5aaVMEX"}},{"cell_type":"code","source":["# Set the train data\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","\n","# Initialize hypothesis function\n","W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","total_epochs = 1000\n","optimizer = torch.optim.SGD([W, b], lr=0.05)\n","\n","# Let's train\n","for epoch in np.arange(total_epochs + 1):\n","\n","  H = W * x_train + b\n","  cost = torch.mean((H - y_train) ** 2)\n","\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  if epoch % 100 == 0:\n","    print(f\"Gradient of W: {W.grad.item():.3f}, Gradient of b: {b.grad.item():.3f}\")\n","    print(f\"Epoch {epoch} - W: {W.item():.3f}, b: {b.item():.3f}, COST : {cost.item():.6f}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"in44v5LT39K8","executionInfo":{"status":"ok","timestamp":1688709558360,"user_tz":-540,"elapsed":416,"user":{"displayName":"곽성수","userId":"13238959409532928202"}},"outputId":"8e52c04a-51c5-46ac-8750-8e896d2138b4"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient of W: -18.667, Gradient of b: -8.000\n","Epoch 0 - W: 0.933, b: 0.400, COST : 18.666666\n","\n","Gradient of W: -0.023, Gradient of b: 0.053\n","Epoch 100 - W: 1.904, b: 0.217, COST : 0.006942\n","\n","Gradient of W: -0.007, Gradient of b: 0.016\n","Epoch 200 - W: 1.971, b: 0.065, COST : 0.000618\n","\n","Gradient of W: -0.002, Gradient of b: 0.005\n","Epoch 300 - W: 1.991, b: 0.019, COST : 0.000055\n","\n","Gradient of W: -0.001, Gradient of b: 0.001\n","Epoch 400 - W: 1.997, b: 0.006, COST : 0.000005\n","\n","Gradient of W: -0.000, Gradient of b: 0.000\n","Epoch 500 - W: 1.999, b: 0.002, COST : 0.000000\n","\n","Gradient of W: -0.000, Gradient of b: 0.000\n","Epoch 600 - W: 2.000, b: 0.001, COST : 0.000000\n","\n","Gradient of W: -0.000, Gradient of b: 0.000\n","Epoch 700 - W: 2.000, b: 0.000, COST : 0.000000\n","\n","Gradient of W: -0.000, Gradient of b: 0.000\n","Epoch 800 - W: 2.000, b: 0.000, COST : 0.000000\n","\n","Gradient of W: -0.000, Gradient of b: 0.000\n","Epoch 900 - W: 2.000, b: 0.000, COST : 0.000000\n","\n","Gradient of W: -0.000, Gradient of b: 0.000\n","Epoch 1000 - W: 2.000, b: 0.000, COST : 0.000000\n","\n"]}]},{"cell_type":"markdown","source":["### Multivariate Linear Regression\n","\n","Hypothesis: $$ H(x) = W_1 x_1 + W_2 x_2 + W_3 x_3 + b = W X + b \\text{  }(W = (W_1, W_2, W_3), X = (x_1, x_2, x_3))$$\n","Cost Function: $$ Cost = {1 \\over N} \\sum_{i=1}^{N} {(WX + b - Y) ^ 2} $$"],"metadata":{"id":"EzUqLUaCeSGE"}},{"cell_type":"code","source":["# Set the train data\n","x_train  =  torch.FloatTensor([[73,  80,  75],\n","                               [93,  88,  93],\n","                               [89,  91,  80],\n","                               [96,  98,  100],\n","                               [73,  66,  70]])\n","\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n","\n","# Initialize hypothesis function\n","W = torch.zeros((3,1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","total_epochs = 200\n","optimizer = torch.optim.SGD([W, b], lr=1e-5)\n","\n","# Let's train\n","for epoch in np.arange(total_epochs + 1):\n","\n","  H = x_train.matmul(W) + b\n","  cost = torch.mean((H - y_train) ** 2)\n","\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  if epoch % 20 == 0:\n","    print(f\"Gradient of W1: {W.grad[0].item():.3f}, Gradient of W2: {W.grad[1].item():.3f}, Gradient of W2: {W.grad[2].item():.3f}, Gradient of b: {b.grad.item():.3f}\")\n","    print(f\"Epoch {epoch} - W1: {W[0].item():.3f}, W2: {W[1].item():.3f}, W3: {W[2].item():.3f}, b: {b.item():.3f}, COST : {cost.item():.6f}\\n\")\n"],"metadata":{"id":"9TaHlGtAVKDN","executionInfo":{"status":"ok","timestamp":1688709652934,"user_tz":-540,"elapsed":377,"user":{"displayName":"곽성수","userId":"13238959409532928202"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"49fb0abf-3690-41a3-da5e-5e02bbd3daf5"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient of W1: -29401.201, Gradient of W2: -29360.000, Gradient of W2: -29018.000, Gradient of b: -342.000\n","Epoch 0 - W1: 0.294, W2: 0.294, W3: 0.290, b: 0.003, COST : 29661.800781\n","\n","Gradient of W1: -9.792, Gradient of W2: -3.385, Gradient of W2: 12.293, Gradient of b: -0.103\n","Epoch 20 - W1: 0.681, W2: 0.679, W3: 0.668, b: 0.008, COST : 5.957089\n","\n","Gradient of W1: -9.415, Gradient of W2: -3.001, Gradient of W2: 12.567, Gradient of b: -0.098\n","Epoch 40 - W1: 0.683, W2: 0.679, W3: 0.665, b: 0.008, COST : 5.905738\n","\n","Gradient of W1: -9.385, Gradient of W2: -2.963, Gradient of W2: 12.499, Gradient of b: -0.098\n","Epoch 60 - W1: 0.684, W2: 0.680, W3: 0.663, b: 0.008, COST : 5.854844\n","\n","Gradient of W1: -9.352, Gradient of W2: -2.923, Gradient of W2: 12.434, Gradient of b: -0.097\n","Epoch 80 - W1: 0.686, W2: 0.680, W3: 0.660, b: 0.008, COST : 5.804478\n","\n","Gradient of W1: -9.326, Gradient of W2: -2.890, Gradient of W2: 12.362, Gradient of b: -0.096\n","Epoch 100 - W1: 0.688, W2: 0.681, W3: 0.658, b: 0.008, COST : 5.754573\n","\n","Gradient of W1: -9.294, Gradient of W2: -2.853, Gradient of W2: 12.297, Gradient of b: -0.096\n","Epoch 120 - W1: 0.690, W2: 0.681, W3: 0.655, b: 0.008, COST : 5.705184\n","\n","Gradient of W1: -9.266, Gradient of W2: -2.818, Gradient of W2: 12.228, Gradient of b: -0.095\n","Epoch 140 - W1: 0.692, W2: 0.682, W3: 0.653, b: 0.008, COST : 5.656279\n","\n","Gradient of W1: -9.235, Gradient of W2: -2.781, Gradient of W2: 12.163, Gradient of b: -0.094\n","Epoch 160 - W1: 0.694, W2: 0.683, W3: 0.650, b: 0.008, COST : 5.607846\n","\n","Gradient of W1: -9.205, Gradient of W2: -2.745, Gradient of W2: 12.098, Gradient of b: -0.094\n","Epoch 180 - W1: 0.696, W2: 0.683, W3: 0.648, b: 0.008, COST : 5.559882\n","\n","Gradient of W1: -9.176, Gradient of W2: -2.710, Gradient of W2: 12.032, Gradient of b: -0.093\n","Epoch 200 - W1: 0.697, W2: 0.684, W3: 0.645, b: 0.008, COST : 5.512386\n","\n"]}]},{"cell_type":"markdown","source":["### Using Module Class for Training"],"metadata":{"id":"L2E1BsyFDKm5"}},{"cell_type":"code","source":["# Set the train data\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","\n","# Define model class\n","class UnivariateLinearRegression(torch.nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.linear1 = torch.nn.Linear(1,1)\n","\n","  def forward(self, x):\n","    return self.linear1(x)\n","\n","model = UnivariateLinearRegression()\n","\n","total_epochs = 1000\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n","\n","# Let's train\n","for epoch in np.arange(total_epochs + 1):\n","\n","  pred = model(x_train)\n","  cost = torch.nn.functional.mse_loss(pred, y_train)\n","\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  if epoch % 100 == 0:\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, total_epochs, cost.item()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X7ZYGzO8u9WZ","executionInfo":{"status":"ok","timestamp":1688709619264,"user_tz":-540,"elapsed":353,"user":{"displayName":"곽성수","userId":"13238959409532928202"}},"outputId":"a539b9b1-d1b4-4a32-c916-d3ce7425fecf"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/1000 Cost: 14.092372\n","Epoch  100/1000 Cost: 0.000928\n","Epoch  200/1000 Cost: 0.000083\n","Epoch  300/1000 Cost: 0.000007\n","Epoch  400/1000 Cost: 0.000001\n","Epoch  500/1000 Cost: 0.000000\n","Epoch  600/1000 Cost: 0.000000\n","Epoch  700/1000 Cost: 0.000000\n","Epoch  800/1000 Cost: 0.000000\n","Epoch  900/1000 Cost: 0.000000\n","Epoch 1000/1000 Cost: 0.000000\n"]}]},{"cell_type":"code","source":["# Set the train data\n","x_train  =  torch.FloatTensor([[73,  80,  75],\n","                               [93,  88,  93],\n","                               [89,  91,  80],\n","                               [96,  98,  100],\n","                               [73,  66,  70]])\n","\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n","\n","\n","# Define model class\n","class MultivariateLinearRegression(torch.nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.linear1 = torch.nn.Linear(3,1)\n","\n","  def forward(self, x):\n","    return self.linear1(x)\n","\n","model = MultivariateLinearRegression()\n","\n","total_epochs = 200\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n","\n","# Let's train\n","for epoch in np.arange(total_epochs + 1):\n","\n","  pred = model(x_train)\n","  cost = torch.nn.functional.mse_loss(pred, y_train)\n","\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  if epoch % 20 == 0:\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, total_epochs, cost.item()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0hPTG0Mmu-j7","executionInfo":{"status":"ok","timestamp":1688709636181,"user_tz":-540,"elapsed":359,"user":{"displayName":"곽성수","userId":"13238959409532928202"}},"outputId":"b46d3079-3951-401e-a14e-3035a378c2fd"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/200 Cost: 25607.138672\n","Epoch   20/200 Cost: 0.738754\n","Epoch   40/200 Cost: 0.732838\n","Epoch   60/200 Cost: 0.726994\n","Epoch   80/200 Cost: 0.721217\n","Epoch  100/200 Cost: 0.715500\n","Epoch  120/200 Cost: 0.709863\n","Epoch  140/200 Cost: 0.704260\n","Epoch  160/200 Cost: 0.698734\n","Epoch  180/200 Cost: 0.693273\n","Epoch  200/200 Cost: 0.687843\n"]}]},{"cell_type":"markdown","source":["### Using Dataset Loader for batch training"],"metadata":{"id":"4DSqQ3ZXDUAj"}},{"cell_type":"code","source":["# Set the train data\n","x_train  =  torch.FloatTensor([[73,  80,  75],\n","                               [93,  88,  93],\n","                               [89,  91,  80],\n","                               [96,  98,  100],\n","                               [73,  66,  70]])\n","\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n","\n","\n","# Define model class\n","\n","class MultivariateLinearRegression(torch.nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.linear1 = torch.nn.Linear(3,1)\n","\n","  def forward(self, x):\n","    return self.linear1(x)\n","\n","model = MultivariateLinearRegression()\n","\n","total_epochs = 200\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n","\n","dataset = torch.utils.data.TensorDataset(x_train, y_train)\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n","\n","# Let's train\n","for epoch in np.arange(total_epochs + 1):\n","\n","  for batch_idx, sample in enumerate(dataloader):\n","\n","    _x_train, _y_train = sample\n","\n","    pred = model(_x_train)\n","    cost = torch.nn.functional.mse_loss(pred, _y_train)\n","\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    if epoch % 20 == 0:\n","        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(epoch, total_epochs, batch_idx+1, len(dataloader), cost.item()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cMKPWSwS0oXN","executionInfo":{"status":"ok","timestamp":1688711827814,"user_tz":-540,"elapsed":675,"user":{"displayName":"곽성수","userId":"13238959409532928202"}},"outputId":"8b7b7886-3907-4f21-ecd5-003f99946923"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/200 Batch 1/3 Cost: 43983.636719\n","Epoch    0/200 Batch 2/3 Cost: 10469.808594\n","Epoch    0/200 Batch 3/3 Cost: 2359.473145\n","Epoch   20/200 Batch 1/3 Cost: 12.577541\n","Epoch   20/200 Batch 2/3 Cost: 6.700894\n","Epoch   20/200 Batch 3/3 Cost: 3.782163\n","Epoch   40/200 Batch 1/3 Cost: 2.271132\n","Epoch   40/200 Batch 2/3 Cost: 18.688189\n","Epoch   40/200 Batch 3/3 Cost: 15.491172\n","Epoch   60/200 Batch 1/3 Cost: 10.025870\n","Epoch   60/200 Batch 2/3 Cost: 10.498519\n","Epoch   60/200 Batch 3/3 Cost: 1.312957\n","Epoch   80/200 Batch 1/3 Cost: 5.580003\n","Epoch   80/200 Batch 2/3 Cost: 8.246863\n","Epoch   80/200 Batch 3/3 Cost: 20.136999\n","Epoch  100/200 Batch 1/3 Cost: 5.621683\n","Epoch  100/200 Batch 2/3 Cost: 8.677119\n","Epoch  100/200 Batch 3/3 Cost: 14.103614\n","Epoch  120/200 Batch 1/3 Cost: 5.966008\n","Epoch  120/200 Batch 2/3 Cost: 17.941235\n","Epoch  120/200 Batch 3/3 Cost: 1.483307\n","Epoch  140/200 Batch 1/3 Cost: 8.140509\n","Epoch  140/200 Batch 2/3 Cost: 9.695632\n","Epoch  140/200 Batch 3/3 Cost: 9.819104\n","Epoch  160/200 Batch 1/3 Cost: 12.213753\n","Epoch  160/200 Batch 2/3 Cost: 6.151551\n","Epoch  160/200 Batch 3/3 Cost: 11.209254\n","Epoch  180/200 Batch 1/3 Cost: 8.005898\n","Epoch  180/200 Batch 2/3 Cost: 6.643452\n","Epoch  180/200 Batch 3/3 Cost: 13.727717\n","Epoch  200/200 Batch 1/3 Cost: 7.900016\n","Epoch  200/200 Batch 2/3 Cost: 9.863483\n","Epoch  200/200 Batch 3/3 Cost: 0.910048\n"]}]}]}